from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import transformers
from alphawave_pyexts import serverUtils as sv
from alphawave_pyexts import modelling_RW

if __name__ == '__main__':
    model_name = "tiiuae/falcon-7b-instruct"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = modelling_RW.RWForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
    )
    model.tie_weights()
    print(model)
    print(f"Successfully loaded the model {model_name} into memory")
    print('**** ready to serve on port 5004')

    sv.server_program(model=model, tokenizer=tokenizer)
